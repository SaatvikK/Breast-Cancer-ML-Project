{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "def dataClean(df):\n",
    "  # there's an extra column at the end of the data set that needs removing.\n",
    "  df = df.drop(columns=[\"id\", \"Unnamed: 32\"])\n",
    "  # other than this, there is no more data cleaning to do.\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we isolate the classification column of the datasset ('diagnosis').\n",
    "# Because 'diagnosis' uses M & B as labels and CFS uses numbers, we map\n",
    "# M and B to 1 and 0, respectively.\n",
    "def predClassMapping(df):\n",
    "  df['diagnosis'] = pd.Series(df.diagnosis).map({'M':1,'B':0});\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12/24\n",
    "# CORRELATION FEATURE SELECTION\n",
    "R_XY = None\n",
    "# tauRed = 0.8, k = 6\n",
    "def corrFeatureSelection(df, k = 10, tauRedundancy = 0.8):\n",
    "  # 1) Sort features by absolute correlation with the label (descending)\n",
    "  targetCorr = df.corr()['diagnosis'].abs().sort_values(ascending=False)\n",
    "  R_XY = targetCorr\n",
    "  # 2) Now pick features one by one from the most strongly correlated\n",
    "  #    to the least, but skip any feature that is \"too correlated\"\n",
    "\n",
    "  selectedFeatures = []\n",
    "  rejectedFeatures = []\n",
    "\n",
    "  for feature in targetCorr.index:\n",
    "    if feature == 'diagnosis':\n",
    "      continue  # Skip the label itself\n",
    "\n",
    "    # Check correlation with already selected features\n",
    "    aboveThreshold = False\n",
    "    for alreadySelected in selectedFeatures:\n",
    "      # If the correlation is above the threshold, skip\n",
    "      if abs(df[feature].corr(df[alreadySelected])) > tauRedundancy:\n",
    "        aboveThreshold = True\n",
    "        rejectedFeatures.append(feature)\n",
    "        break\n",
    "\n",
    "    if not aboveThreshold:\n",
    "      selectedFeatures.append(feature)\n",
    "\n",
    "    # If we already have our 10 features, stop\n",
    "    if len(selectedFeatures) == k:\n",
    "      break\n",
    "\n",
    "  print(\"Selected features:\", selectedFeatures)\n",
    "  print(\"Num features:\", len(selectedFeatures))\n",
    "  # This will give up to 10 features that are:\n",
    "  # - highly correlated with the label (because we started with that sorted list),\n",
    "  # - but have low correlation with each other (due to our threshold check).\n",
    "\n",
    "  selectedDF = df[['diagnosis'] + selectedFeatures]\n",
    "\n",
    "  return { \"sfCorrMatrix\": selectedDF.corr(), \"selectedDF\": selectedDF,\"selectedFeatures\": selectedFeatures, \"rejectedFeatures\": rejectedFeatures }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load and process data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = dataClean(df)\n",
    "df = predClassMapping(df)\n",
    "\n",
    "# Compute correlations and perform CFS\n",
    "R_XY = df.corr()['diagnosis'].abs().sort_values(ascending=False)\n",
    "# Remove the 'diagnosis' entry from the correlation Series\n",
    "R_XY = R_XY.drop('diagnosis')\n",
    "\n",
    "res = corrFeatureSelection(df, k=6, tauRedundancy=0.8)\n",
    "\n",
    "# Extract features and their correlation values (without 'diagnosis')\n",
    "features = R_XY.index\n",
    "correlations = R_XY.values\n",
    "\n",
    "# Create a color list for each feature based on its status:\n",
    "colors = []\n",
    "for feature in features:\n",
    "    if feature in res[\"selectedFeatures\"]:\n",
    "        colors.append(\"green\")\n",
    "    elif feature in res[\"rejectedFeatures\"]:\n",
    "        colors.append(\"red\")\n",
    "    else:\n",
    "        # Implicitly rejected (not processed) features\n",
    "        colors.append(\"blue\")\n",
    "\n",
    "# Plot the bar chart with the custom colors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(features, correlations, color=colors)\n",
    "plt.title('Relevancy of each feature', pad=20)\n",
    "plt.ylabel('Pearson Correlation Coefficient (0 to 1)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Create a custom legend to clarify the colors\n",
    "selected_patch = mpatches.Patch(color=\"green\", label=\"Selected Features\")\n",
    "rejected_patch = mpatches.Patch(color=\"red\", label=\"Rejected Features\")\n",
    "implicit_patch = mpatches.Patch(color=\"blue\", label=\"Implicitly Rejected Features\")\n",
    "plt.legend(handles=[selected_patch, rejected_patch, implicit_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/24\n",
    "# SPLIT DATA SET\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def splitData(df, testSize = 0.20):\n",
    "  # Split the label column from the features\n",
    "  Y = df.loc[:, 'diagnosis']\n",
    "  X = df.loc[:, df.columns != 'diagnosis']\n",
    "\n",
    "  # Now split the X and Y datasets into train/test (0.8/0.2 split by default)\n",
    "  Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=testSize, random_state=23)\n",
    "  return [Xtrain, Xtest, Ytrain, Ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the features in a given dataframe set. \n",
    "# 26/02/2025\n",
    "import math\n",
    "\n",
    "def plot_all_features(df):\n",
    "  # Select only numeric columns (ignoring non-numeric ones like 'diagnosis')\n",
    "  numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "  print(\"Numeric columns to plot:\", numeric_cols)\n",
    "  \n",
    "  # Determine subplot grid size: 3 columns per row\n",
    "  num_plots = len(numeric_cols)\n",
    "  ncols = 3\n",
    "  nrows = math.ceil(num_plots / ncols)\n",
    "  \n",
    "  fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows), squeeze=False)\n",
    "  axes = axes.flatten()\n",
    "  \n",
    "  # Create a line plot for each numeric feature\n",
    "  for i, col in enumerate(numeric_cols):\n",
    "    axes[i].plot(df.index, df[col], marker='o', linestyle='-')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Index')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].grid(True)\n",
    "  \n",
    "  # Remove any empty subplots if grid has extra axes\n",
    "  for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  print(type(df.iloc[0]))\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIVE SMOOTHING\n",
    "#26/02/2025\n",
    "\n",
    "def additiveSmoothing(x_i: np.ndarray, alpha: float) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  x_i   - a NumPy array where the first element is a diagnosis (or label)\n",
    "          and the remaining elements are numeric counts.\n",
    "  alpha - the pseudocount (> 0; if alpha is 0, no smoothing is applied)\n",
    "  \n",
    "  The smoothed value for each numeric element (x_j, for j >= 1) is computed as:\n",
    "    x_j_new = N * ((x_j + alpha) / (N + alpha * d))\n",
    "  where:\n",
    "    N = sum of numeric counts (x_i[1:]),\n",
    "    d = number of numeric features (length of x_i[1:]).\n",
    "    \n",
    "  The first element (diagnosis) is left unchanged.\n",
    "  \"\"\"\n",
    "\n",
    "  # Work on a copy to avoid modifying the original array\n",
    "  x_iCopy = x_i.copy()\n",
    "  \n",
    "  # Extract the diagnosis (first element)\n",
    "  diagnosis = x_iCopy[0]\n",
    "  \n",
    "  # Extract the numeric part, converting to float if necessary\n",
    "  x_iCopy = x_iCopy[1:].astype(float)\n",
    "  \n",
    "  # Calculate the total count (N) and the number of features (d)\n",
    "  N = x_iCopy.sum()\n",
    "  d = len(x_iCopy)\n",
    "  \n",
    "  # Compute the smoothed numeric values using vectorized operations\n",
    "  x_iNew = N * (x_iCopy + alpha) / (N + alpha * d)\n",
    "  \n",
    "  # Combine the unchanged diagnosis with the smoothed values\n",
    "  result = np.concatenate(([diagnosis], x_iNew))\n",
    "  return result\n",
    "\n",
    "def dataSmooth(df, alpha):\n",
    "  for i in range(len(df)):\n",
    "    x_i = df.iloc[i].to_numpy()\n",
    "    x_iNew = additiveSmoothing(x_i, alpha)\n",
    "    df.iloc[i] = x_iNew\n",
    "  \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26/02/2025\n",
    "def smoothTestFunc():\n",
    "  # Load the CSV file\n",
    "  df = pd.read_csv(\"data.csv\")\n",
    "  df = dataClean(df)\n",
    "  df = predClassMapping(df)\n",
    "\n",
    "  plot_all_features(df)\n",
    "  print(\"==================== AFTER SMOOTHING================================\")\n",
    "  for i in range(len(df)):\n",
    "    x_i = df.iloc[i].to_numpy()\n",
    "    x_iNew = additiveSmoothing(x_i, 100000)\n",
    "    df.iloc[i] = x_iNew\n",
    "\n",
    "  plot_all_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/02/2025\n",
    "# This is the data preprocessing handler. Can be used to execute any one or multiple forms of data preprocessing (cleaning, mapping, feature selection, data splitting).\n",
    "# NOTE: If data set splitting and CFS are to be done in the same call, the CFS option MUST be first in the `processes` array (so that the dataset is feature selected, and THEN split)\n",
    "def dataPreProcessing(dsFile = \"data\", df = None, processes = [\"clean\", \"predMap\", \"CFS\", \"splitSet\"], kFeatures = 10, tauRedundancy = 0.8, testSize = 0.20):\n",
    "  returnVars = {\"df\": None, \"CFS Corr Matrix\": None, \"Xtrain\": None, \"Xtest\": None, \"Ytrain\": None, \"Ytest\": None}\n",
    "  if df is None:\n",
    "    df = pd.read_csv(dsFile + '.csv')\n",
    "\n",
    "  for process in processes:\n",
    "    match process:\n",
    "      case \"clean\":\n",
    "        df = dataClean(df)\n",
    "        df = additiveSmoothing(df, 100000)\n",
    "      case \"predMap\":\n",
    "        df = predClassMapping(df)\n",
    "      case \"CFS\":\n",
    "        res = corrFeatureSelection(df, kFeatures, tauRedundancy)\n",
    "        returnVars[\"CFS Corr Matrix\"] = res[\"sfCorrMatrix\"]\n",
    "        df = res[\"selectedDF\"]\n",
    "      case \"splitSet\":\n",
    "        Xtrain, Xtest, Ytrain, Ytest = splitData(df, testSize)\n",
    "        returnVars[\"Xtrain\"] = Xtrain\n",
    "        returnVars[\"Xtest\"] = Xtest\n",
    "        returnVars[\"Ytrain\"] = Ytrain\n",
    "        returnVars[\"Ytest\"] = Ytest\n",
    "      case _:\n",
    "        raise Exception(\"Processes param is empty. Options: ['clean', 'predMap', 'CFS', 'splitSet']\")\n",
    "\n",
    "  returnVars[\"df\"] = df\n",
    "  return returnVars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
