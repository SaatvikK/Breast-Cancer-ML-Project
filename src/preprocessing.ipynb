{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "\n",
    "# BASIC PREPROCESSING\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "def dataClean(df, set = 1):\n",
    "  numNANs = df.isna().sum()\n",
    "  print(\"Number of NANs in data set\", set, \":\", numNANs)\n",
    "  print(\"Preparing to interpolate NANs...\")\n",
    "  df = df.interpolate(limit_direction = \"both\")\n",
    "  print(\"Number of NANs now: \", df.isna().sum())\n",
    "  try:\n",
    "    if set == 1:\n",
    "      # NOTE: the extra column only exists in the first dataset (\"data.csv\")\n",
    "      # there's an extra column at the end of the data set that needs removing.\n",
    "      df = df.drop(columns=[\"id\", \"Unnamed: 32\"])\n",
    "      # other than this, there is no more data cleaning to do.\n",
    "  except: pass;\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we isolate the classification column of the datasset ('diagnosis').\n",
    "# Because 'diagnosis' uses M & B as labels and CFS uses numbers, we map\n",
    "# M (or 1') and B (or -1') to 1 and 0, respectively.\n",
    "def predClassMapping(df, set = 1):\n",
    "  if set == 1: #data set 1\n",
    "    df['diagnosis'] = pd.Series(df.diagnosis).map({'M':1,'B':0});\n",
    "  elif set == 2:\n",
    "    df[\"diagnosis\"] = pd.Series(df.diagnosis).map({\"1'\": 1, \"-1'\": 0})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "\n",
    "# FEATURE SELECTION\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12/24\n",
    "# CORRELATION FEATURE SELECTION\n",
    "R_XY = None\n",
    "# tauRed = 0.8, k = 6\n",
    "def corrFeatureSelection(df, k = 10, tauRedundancy = 0.8):\n",
    "  # 1) Sort features by absolute correlation with the label (descending)\n",
    "  targetCorr = df.corr()['diagnosis'].abs().sort_values(ascending=False)\n",
    "  R_XY = targetCorr\n",
    "  # 2) Now pick features one by one from the most strongly correlated\n",
    "  #    to the least, but skip any feature that is \"too correlated\"\n",
    "\n",
    "  selectedFeatures = []\n",
    "  rejectedFeatures = []\n",
    "\n",
    "  for feature in targetCorr.index:\n",
    "    if feature == 'diagnosis':\n",
    "      continue  # Skip the label itself\n",
    "\n",
    "    # Check correlation with already selected features\n",
    "    aboveThreshold = False\n",
    "    for alreadySelected in selectedFeatures:\n",
    "      # If the correlation is above the threshold, skip\n",
    "      if abs(df[feature].corr(df[alreadySelected])) > tauRedundancy:\n",
    "        aboveThreshold = True\n",
    "        rejectedFeatures.append(feature)\n",
    "        break\n",
    "\n",
    "    if not aboveThreshold:\n",
    "      selectedFeatures.append(feature)\n",
    "\n",
    "    # If we already have our 10 features, stop\n",
    "    if len(selectedFeatures) == k:\n",
    "      break\n",
    "\n",
    "  print(\"Selected features:\", selectedFeatures)\n",
    "  print(\"Num features:\", len(selectedFeatures))\n",
    "  # This will give up to 10 features that are:\n",
    "  # - highly correlated with the label (because we started with that sorted list),\n",
    "  # - but have low correlation with each other (due to our threshold check).\n",
    "\n",
    "  selectedDF = df[['diagnosis'] + selectedFeatures]\n",
    "\n",
    "  return { \"sfCorrMatrix\": selectedDF.corr(), \"selectedDF\": selectedDF,\"selectedFeatures\": selectedFeatures, \"rejectedFeatures\": rejectedFeatures }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# RELEVANCY GRAPH\\n\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as mpatches\\n\\n# Load and process data\\ndf = pd.read_csv(\"data2.csv\")\\ndf = dataClean(df, set = 2)\\ndf = predClassMapping(df, set = 2)\\n\\n# Compute correlations and perform CFS\\nR_XY = df.corr()[\\'diagnosis\\'].abs().sort_values(ascending=False)\\n# Remove the \\'diagnosis\\' entry from the correlation Series\\nR_XY = R_XY.drop(\\'diagnosis\\')\\n\\nres = corrFeatureSelection(df, k=6, tauRedundancy=0.8)\\n\\n# Extract features and their correlation values (without \\'diagnosis\\')\\nfeatures = R_XY.index\\ncorrelations = R_XY.values\\n\\n# Create a color list for each feature based on its status:\\ncolors = []\\nfor feature in features:\\n    if feature in res[\"selectedFeatures\"]:\\n        colors.append(\"green\")\\n    elif feature in res[\"rejectedFeatures\"]:\\n        colors.append(\"red\")\\n    else:\\n        # Implicitly rejected (not processed) features\\n        colors.append(\"blue\")\\n\\n# Plot the bar chart with the custom colors\\nplt.figure(figsize=(12, 6))\\nplt.bar(features, correlations, color=colors)\\nplt.title(\\'Relevancy of each feature\\', pad=20)\\nplt.ylabel(\\'Pearson Correlation Coefficient (0 to 1)\\')\\nplt.xticks(rotation=90)\\n\\n# Create a custom legend to clarify the colors\\nselected_patch = mpatches.Patch(color=\"green\", label=\"Selected Features\")\\nrejected_patch = mpatches.Patch(color=\"red\", label=\"Rejected Features\")\\nimplicit_patch = mpatches.Patch(color=\"blue\", label=\"Implicitly Rejected Features\")\\nplt.legend(handles=[selected_patch, rejected_patch, implicit_patch])\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# RELEVANCY GRAPH\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load and process data\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df = dataClean(df, set = 2)\n",
    "df = predClassMapping(df, set = 2)\n",
    "\n",
    "# Compute correlations and perform CFS\n",
    "R_XY = df.corr()['diagnosis'].abs().sort_values(ascending=False)\n",
    "# Remove the 'diagnosis' entry from the correlation Series\n",
    "R_XY = R_XY.drop('diagnosis')\n",
    "\n",
    "res = corrFeatureSelection(df, k=6, tauRedundancy=0.8)\n",
    "\n",
    "# Extract features and their correlation values (without 'diagnosis')\n",
    "features = R_XY.index\n",
    "correlations = R_XY.values\n",
    "\n",
    "# Create a color list for each feature based on its status:\n",
    "colors = []\n",
    "for feature in features:\n",
    "    if feature in res[\"selectedFeatures\"]:\n",
    "        colors.append(\"green\")\n",
    "    elif feature in res[\"rejectedFeatures\"]:\n",
    "        colors.append(\"red\")\n",
    "    else:\n",
    "        # Implicitly rejected (not processed) features\n",
    "        colors.append(\"blue\")\n",
    "\n",
    "# Plot the bar chart with the custom colors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(features, correlations, color=colors)\n",
    "plt.title('Relevancy of each feature', pad=20)\n",
    "plt.ylabel('Pearson Correlation Coefficient (0 to 1)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Create a custom legend to clarify the colors\n",
    "selected_patch = mpatches.Patch(color=\"green\", label=\"Selected Features\")\n",
    "rejected_patch = mpatches.Patch(color=\"red\", label=\"Rejected Features\")\n",
    "implicit_patch = mpatches.Patch(color=\"blue\", label=\"Implicitly Rejected Features\")\n",
    "plt.legend(handles=[selected_patch, rejected_patch, implicit_patch])\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/24\n",
    "# SPLIT DATA SET\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def splitData(df, testSize = 0.20):\n",
    "  # Split the label column from the features\n",
    "  Y = df.loc[:, 'diagnosis']\n",
    "  X = df.loc[:, df.columns != 'diagnosis']\n",
    "\n",
    "  if testSize != 0.0: # We only want to do this if we want to split it into training and testing (non domain adaptation running)\n",
    "    # Now split the X and Y datasets into train/test (0.8/0.2 split by default)\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=testSize, random_state=23)\n",
    "    print(type(Xtrain))\n",
    "    return [Xtrain, Xtest, Ytrain, Ytest]\n",
    "  else:\n",
    "    return [X, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN CENTERING\n",
    "def centreMean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  features = df.columns.to_list()\n",
    "  if \"diagnosis\" in features: raise Exception(\"Cannot zero-mean a dataset with the classifications.\")\n",
    "  newDF = df.copy()\n",
    "  # find means\n",
    "  means = []\n",
    "  for feature in features:\n",
    "    means.append(df.loc[:, feature].mean())\n",
    "  \n",
    "  for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[1]):\n",
    "      x_ij = df.iloc[i,j]\n",
    "      mean = means[j]\n",
    "      zeroed_x_ij = x_ij - mean\n",
    "      newDF.iloc[i,j] = zeroed_x_ij\n",
    "  \n",
    "  return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "\n",
    "# DEBIASING\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.debiasing import XYsplit, XYmerge, rndSample, smoteSample, adasynSample\n",
    "def debiasData(df: pd.DataFrame, technique = \"smote\") -> pd.DataFrame:\n",
    "  X, Y = XYsplit(df)\n",
    "  newX, newY = None, None\n",
    "  match(technique):\n",
    "    case \"over\":\n",
    "      newX, newY = rndSample(X, Y, mode = \"over\")\n",
    "    case \"under\":\n",
    "      newX, newY = rndSample(X, Y, mode = \"under\")\n",
    "    case \"smote\":\n",
    "      newX, newY = smoteSample(X, Y)\n",
    "    case \"adasyn\":\n",
    "      newX, newY = adasynSample(X, Y)\n",
    "    case _: \n",
    "      raise Exception(\"technique parameter is empty. must be one of: ['over', 'under', 'smote', 'adasyn']\")\n",
    "  \n",
    "  df = XYmerge(newX, newY)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/02/2025\n",
    "# This is the data preprocessing handler. Can be used to execute any one or multiple forms of data preprocessing (cleaning, mapping, feature selection, data splitting).\n",
    "# NOTE: If data set splitting and CFS are to be done in the same call, the CFS option MUST be first in the `processes` array (so that the dataset is feature selected, and THEN split)\n",
    "def dataPreProcessing(\n",
    "    dsFile = \"../data/data.csv\", Dset = 1, df = None, processes = [\"clean\", \"predMap\", \"CFS\", \"centreMean\", \"splitSet\", \"debiasing\"], \n",
    "    kFeatures = 10, tauRedundancy = 0.8, testSize = 0.20, debiasTechnique = \"smote\"\n",
    "):\n",
    "  X, Y = None, None\n",
    "  returnVars = {} #{\"df\": None, \"CFS Corr Matrix\": None, \"Xtrain\": None, \"Xtest\": None, \"Ytrain\": None, \"Ytest\": None}\n",
    "  if df is None:\n",
    "    df = pd.read_csv(dsFile + '.csv')\n",
    "  for process in processes:\n",
    "    match process:\n",
    "      case \"clean\":\n",
    "        df = dataClean(df, Dset)\n",
    "      case \"predMap\":\n",
    "        df = predClassMapping(df, Dset)\n",
    "      case \"CFS\":\n",
    "        res = corrFeatureSelection(df, kFeatures, tauRedundancy)\n",
    "        returnVars[\"CFS Corr Matrix\"] = res[\"sfCorrMatrix\"]\n",
    "        df = res[\"selectedDF\"]\n",
    "      case \"centreMean\": # USED WITH DOMAIN ADAPTATION\n",
    "        classes = df.loc[:, \"diagnosis\"]\n",
    "        df = df.drop([\"diagnosis\"], axis = 1)\n",
    "        df = centreMean(df)\n",
    "        df[\"diagnosis\"] = classes\n",
    "      case \"splitSet\":\n",
    "        if testSize == 0.0:\n",
    "          X, Y = splitData(df, testSize)\n",
    "          returnVars[\"X\"], returnVars[\"Y\"] = X, Y\n",
    "        else:\n",
    "          Xtrain, Xtest, Ytrain, Ytest = splitData(df, testSize)\n",
    "          returnVars[\"Xtrain\"] = Xtrain\n",
    "          returnVars[\"Xtest\"] = Xtest\n",
    "          returnVars[\"Ytrain\"] = Ytrain\n",
    "          returnVars[\"Ytest\"] = Ytest\n",
    "      case \"debiasing\":\n",
    "\n",
    "        if testSize != 0.0: # if NOT doing domain adaptation\n",
    "          X, Y = Xtrain.copy(), Ytrain.copy() # we want to rename Xtrain and Ytrain so it fits with the following code (i.e. generalise it)\n",
    "        # Convert them both to dataframes and combine them\n",
    "\n",
    "        df = X.copy()\n",
    "        df[\"diagnosis\"] = Y\n",
    "        df = debiasData(df, technique =  debiasTechnique)\n",
    "\n",
    "        # Split the debiased dataframe\n",
    "        dfY = df['diagnosis']\n",
    "        dfX = df.drop('diagnosis', axis = 1)\n",
    "\n",
    "        label_counts = dfY.value_counts()\n",
    "\n",
    "        # Print the counts\n",
    "        print(\"DEBIASING USING:\", debiasTechnique)\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count} rows\")\n",
    "        # Now add them back to `returnVars` under the correct key (depending on if we're doing DA or not).\n",
    "        if testSize != 0.0:\n",
    "          returnVars[\"Xtrain\"], returnVars[\"Ytrain\"] = dfX, dfY\n",
    "        else:\n",
    "          returnVars[\"X\"], returnVars[\"Y\"] = dfX, dfY\n",
    "      case _:\n",
    "        raise Exception(\"Processes param is empty. Options: ['clean', 'predMap', 'CFS', 'splitSet']\")\n",
    "\n",
    "  returnVars[\"df\"] = df\n",
    "  return returnVars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
