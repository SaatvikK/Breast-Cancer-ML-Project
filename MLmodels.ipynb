{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.performance import performanceEval as perf\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class baseModel:\n",
    "  def __init__(self, model, modelName):\n",
    "    self.model = model\n",
    "    self.modelName = modelName\n",
    "  \n",
    "  def train(self, Xtrain, Ytrain):\n",
    "    self.model.fit(Xtrain, Ytrain)\n",
    "    if self.modelName == \"dtModel\": \n",
    "      plt.figure(figsize=(20, 10), dpi=200)  # bigger figure and higher DPI\n",
    "      tree.plot_tree(self.model, filled=True)\n",
    "      plt.show()\n",
    "    return [\"training complete\", self]\n",
    "  \n",
    "  def test(self, Xtest):\n",
    "    Ypred = self.model.predict(Xtest)\n",
    "    return Ypred\n",
    "  \n",
    "  def perfEval(self, Ypred, Ytest):\n",
    "    return perf(Ypred, Ytest, self.modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/24\n",
    "######### LOGISTIC REGRESSION #########\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class logReg(baseModel):\n",
    "  def __init__(self):\n",
    "    super().__init__(LogisticRegression(random_state=0), \"regModel\")\n",
    "  \n",
    "\n",
    "  def computeClassBound3D(self, df):\n",
    "    df = df.drop('diagnosis', axis=1)\n",
    "    params = self.model.coef_[0] # get the SKL learning parameters\n",
    "    intercept = self.model.intercept_[0]\n",
    "    sampleX0, sampleX1, means = self.findSampleX(df)\n",
    "\n",
    "    # Because we'll be plotting onto a 3D surface, we convert sampleX0 and X1 to meshgrids\n",
    "    X0, X1 = np.meshgrid(sampleX0, sampleX1)\n",
    "\n",
    "    numerator = 0\n",
    "    # Now compute for X2\n",
    "    for j in range(len(df.columns)):\n",
    "      if j == 0:\n",
    "        numerator += X0*params[0]\n",
    "        continue\n",
    "      elif j == 1:\n",
    "        numerator += X1*params[1]\n",
    "        continue\n",
    "      elif j == 2: continue # as x_2 is the one we're solving for\n",
    "\n",
    "      numerator += params[j]*means[j]\n",
    "\n",
    "    # Solve for X2 using the decision boundary equation:\n",
    "    # intercept + params[0]*X0 + params[1]*X1 + params[2]*X2 + fixed = 0\n",
    "    X2 = -1 * numerator / params[2]\n",
    "\n",
    "    # Take a 3d slice of the feature space\n",
    "    H3D = intercept + params[0]*X0 + params[1]*X1 + params[2]*X2\n",
    "\n",
    "    return [H3D, X0, X1, X2]\n",
    "  \n",
    "  def findSampleX(self, df):\n",
    "    # drop diagnosis\n",
    "    features = df.columns.values.tolist()\n",
    "    means = [] # array to store feature means\n",
    "\n",
    "    # iterate through dataset\n",
    "    for j in range(len(df.columns)):\n",
    "      mean = df.loc[:, features[j]].mean()\n",
    "      means.append(mean) # add mean to means\n",
    "    \n",
    "    sampleX0 = np.linspace(-1, 3, 400)\n",
    "    sampleX1 = np.linspace(-1, 3, 400)\n",
    "    return [sampleX0, sampleX1, means]\n",
    "  \n",
    "  def plotClassBoundary3D(self, df):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    # Get the 2D grid and decision boundary from computeClassBound3D.\n",
    "    # Here, X0, X1, and X2 are already 2D arrays.\n",
    "    H3D, X0, X1, X2 = self.computeClassBound3D(df)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the decision boundary surface (where the decision function is zero)\n",
    "    boundary_surface = ax.plot_surface(X0, X1, X2, cmap='coolwarm', alpha=0.7)\n",
    "    \n",
    "    # Shade the region where the decision function is positive (Y = 1)\n",
    "    # We project a filled contour onto a plane below the surface.\n",
    "    z_offset = np.min(X2) - 1  # adjust this as needed\n",
    "    region_contour = ax.contourf(X0, X1, H3D, zdir='z', offset=z_offset, \n",
    "                                 levels=[0, np.max(H3D)], cmap='Reds', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Feature X0')\n",
    "    ax.set_ylabel('Feature X1')\n",
    "    ax.set_zlabel('Feature X2')\n",
    "    ax.set_title('3D Decision Boundary and Region Y=1')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### SVM #########\n",
    "# https://scikit-learn.org/1.5/modules/svm.html#classification\n",
    "# https://scikit-learn.org/1.5/modules/svm.html#svc (mathematical theory)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "class supVecMac(baseModel):\n",
    "  def __init__(self):\n",
    "    super().__init__(svm.LinearSVC(), \"svmModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Naive Bayes #########\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "class naiBayClass(baseModel):\n",
    "  def __init__(self):\n",
    "    super().__init__(naive_bayes.GaussianNB(), \"nbcModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### K-NN #########\n",
    "# https://scikit-learn.org/1.5/modules/neighbors.html#nearest-neighbors-classification\n",
    "from sklearn import neighbors\n",
    "\n",
    "class kNearNeigh(baseModel):\n",
    "  def __init__(self):\n",
    "    super().__init__(neighbors.KNeighborsClassifier(algorithm = 'ball_tree', metric = 'manhattan'), \"knnModel\")\n",
    "    # Reason for using ball-tree algorithm: https://scikit-learn.org/1.5/modules/neighbors.html#Ball_Tree\n",
    "    # See lit review for the reasoning behind the chosen distance metric\n",
    "    # Also: https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics\n",
    "\n",
    "\n",
    "# Below was last updated 27/12/2024\n",
    "# Manhattan distance: {'accuracy': 96.49122807017544, 'specificity': 100.0, 'sensitivity': 89.74358974358975, 'recall': 89.74358974358975, 'precision': 100.0}\n",
    "# Euclidean distance: {'accuracy': 95.6140350877193, 'specificity': 97.33333333333334, 'sensitivity': 92.3076923076923, 'recall': 92.3076923076923, 'precision': 94.73684210526315}\n",
    "# Cityblock distance: {'accuracy': 96.49122807017544, 'specificity': 100.0, 'sensitivity': 89.74358974358975, 'recall': 89.74358974358975, 'precision': 100.0}\n",
    "# Haversine distance: Not valid for > 2 dimensions\n",
    "# L1 distance: {'accuracy': 96.49122807017544, 'specificity': 100.0, 'sensitivity': 89.74358974358975, 'recall': 89.74358974358975, 'precision': 100.0}\n",
    "# L2 distance: {'accuracy': 95.6140350877193, 'specificity': 97.33333333333334, 'sensitivity': 92.3076923076923, 'recall': 92.3076923076923, 'precision': 94.73684210526315}\n",
    "\n",
    "# From above, can be seen that Manhattan/L1/Cityblock yield the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Decision Tree #########\n",
    "# https://scikit-learn.org/1.5/modules/tree.html#classification\n",
    "# https://scikit-learn.org/1.5/modules/tree.html#mathematical-formulation (mathematical theory)\n",
    "\n",
    "# SKLearn defaultly uses an optimised implementation of the CART decision tree algorithm.\n",
    "from sklearn import tree\n",
    "\n",
    "class decTree(baseModel):\n",
    "  def __init__(self):\n",
    "    super().__init__(tree.DecisionTreeClassifier(criterion='gini', splitter='best'), \"dtModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06/02/2025\n",
    "def runModels(Xtrain, Ytrain, Xtest, modes = [\"train\"], inModels = None):\n",
    "  regModel, svmModel, nbcModel, knnModel, dtModel = logReg(), supVecMac(), naiBayClass(), kNearNeigh(), decTree()\n",
    "  Ypreds = {}\n",
    "  models = {}\n",
    "\n",
    "  if inModels == None and (\"test\" in modes and \"train\" not in modes):\n",
    "      raise Exception(\"Error: Cannot test models without training them. Please either add 'train' to the modes list, or supply the trained models in the `models` param list.\")\n",
    "\n",
    "  models = inModels if inModels != None else {}\n",
    "\n",
    "  if \"train\" in modes and models == {}:\n",
    "    Ypreds[\"Ypred_regModel\"], models[\"regModel\"] = regModel.train(Xtrain, Ytrain)\n",
    "    Ypreds[\"Ypred_svmModel\"], models[\"svmModel\"] = svmModel.train(Xtrain, Ytrain)\n",
    "    Ypreds[\"Ypred_nbcModel\"], models[\"nbcModel\"] = nbcModel.train(Xtrain, Ytrain)\n",
    "    Ypreds[\"Ypred_knnModel\"], models[\"knnModel\"] = knnModel.train(Xtrain, Ytrain)\n",
    "    Ypreds[\"Ypred_dtModel\"], models[\"dtModel\"] = dtModel.train(Xtrain, Ytrain)\n",
    "\n",
    "  if \"test\" in modes:\n",
    "    for model in models:\n",
    "      Ypreds[\"Ypred_\" + model] = models[model].test(Xtest)\n",
    "\n",
    "  return [Ypreds, models]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
